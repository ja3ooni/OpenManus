"""
Test utilities for OpenManus testing framework.
"""

import asyncio
import json
import os
import tempfile
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
from unittest.mock import AsyncMock, Mock

import pytest
from faker import Faker

from app.schema import AgentResponse, ConversationMessage, ToolCall, ToolResult


class TestDataGenerator:
    """Generate test data for various scenarios."""

    def __init__(self):
        self.fake = Faker()

    def generate_conversation_messages(
        self, count: int = 5
    ) -> List[ConversationMessage]:
        """Generate conversation messages."""
        messages = []
        roles = ["user", "assistant"]

        for i in range(count):
            role = roles[i % 2]
            content = self.fake.text() if role == "user" else self.fake.sentence()

            messages.append(
                ConversationMessage(
                    role=role,
                    content=content,
                    timestamp=self.fake.date_time(),
                )
            )

        return messages

    def generate_tool_calls(self, count: int = 3) -> List[ToolCall]:
        """Generate tool calls."""
        tool_names = [
            "file_read",
            "web_search",
            "python_execute",
            "bash",
            "str_replace",
        ]
        calls = []

        for _ in range(count):
            tool_name = self.fake.random_element(tool_names)
            arguments = self._generate_tool_arguments(tool_name)

            calls.append(
                ToolCall(
                    name=tool_name,
                    arguments=arguments,
                    call_id=self.fake.uuid4(),
                )
            )

        return calls

    def _generate_tool_arguments(self, tool_name: str) -> Dict[str, Any]:
        """Generate arguments for specific tools."""
        if tool_name == "file_read":
            return {"path": self.fake.file_path()}
        elif tool_name == "web_search":
            return {"query": self.fake.sentence()}
        elif tool_name == "python_execute":
            return {"code": f"print('{self.fake.sentence()}')"}
        elif tool_name == "bash":
            return {"command": f"echo '{self.fake.word()}'"}
        elif tool_name == "str_replace":
            return {
                "old_str": self.fake.word(),
                "new_str": self.fake.word(),
                "path": self.fake.file_path(),
            }
        else:
            return {"input": self.fake.text()}

    def generate_agent_response(
        self, success: bool = True, include_tool_calls: bool = False
    ) -> AgentResponse:
        """Generate agent response."""
        response = AgentResponse(
            content=self.fake.text(),
            success=success,
            metadata={"test": True, "timestamp": time.time()},
        )

        if not success:
            response.error = self.fake.sentence()

        if include_tool_calls:
            response.tool_calls = self.generate_tool_calls(2)

        return response

    def generate_tool_result(self, success: bool = True) -> ToolResult:
        """Generate tool result."""
        result = ToolResult(
            success=success,
            result=self.fake.text() if success else None,
            metadata={"execution_time": self.fake.random.uniform(0.1, 2.0)},
        )

        if not success:
            result.error = self.fake.sentence()

        return result

    def generate_file_content(self, file_type: str = "text") -> str:
        """Generate file content based on type."""
        if file_type == "json":
            return json.dumps(
                {
                    "name": self.fake.name(),
                    "email": self.fake.email(),
                    "data": [self.fake.word() for _ in range(5)],
                },
                indent=2,
            )
        elif file_type == "csv":
            lines = ["name,email,age"]
            for _ in range(5):
                lines.append(
                    f"{self.fake.name()},{self.fake.email()},{self.fake.random_int(18, 80)}"
                )
            return "\n".join(lines)
        elif file_type == "python":
            return f'''
def test_function():
    """Test function generated by faker."""
    name = "{self.fake.name()}"
    return f"Hello, {{name}}!"

if __name__ == "__main__":
    print(test_function())
'''
        else:
            return self.fake.text()


class MockFactory:
    """Factory for creating various mocks."""

    @staticmethod
    def create_mock_llm_client(responses: Optional[List[str]] = None) -> Mock:
        """Create a mock LLM client."""
        mock_client = Mock()

        if responses is None:
            responses = ["Mock response 1", "Mock response 2"]

        response_iter = iter(responses * 10)  # Repeat responses

        async def mock_create_completion(*args, **kwargs):
            return {
                "choices": [{"message": {"content": next(response_iter)}}],
                "usage": {"total_tokens": 100},
            }

        mock_client.create_completion = AsyncMock(side_effect=mock_create_completion)
        return mock_client

    @staticmethod
    def create_mock_tool(
        name: str,
        responses: Optional[List[Any]] = None,
        errors: Optional[List[Exception]] = None,
    ) -> Mock:
        """Create a mock tool."""
        mock_tool = Mock()
        mock_tool.name = name
        mock_tool.description = f"Mock {name} tool"

        call_count = 0

        async def mock_execute(**kwargs):
            nonlocal call_count

            if errors and call_count < len(errors):
                error = errors[call_count]
                call_count += 1
                raise error

            response_index = min(call_count, len(responses) - 1) if responses else 0
            result = (
                responses[response_index] if responses else f"Mock result from {name}"
            )
            call_count += 1

            return ToolResult(success=True, result=result)

        mock_tool.execute = AsyncMock(side_effect=mock_execute)
        return mock_tool

    @staticmethod
    def create_mock_agent(responses: Optional[List[str]] = None) -> Mock:
        """Create a mock agent."""
        mock_agent = Mock()

        if responses is None:
            responses = ["Mock agent response"]

        response_iter = iter(responses * 10)

        async def mock_process_message(message: str):
            return AgentResponse(
                content=next(response_iter),
                success=True,
                metadata={"mock": True},
            )

        mock_agent.process_message = AsyncMock(side_effect=mock_process_message)
        return mock_agent


class FileTestHelper:
    """Helper for file-related testing operations."""

    def __init__(self, temp_dir: Optional[Path] = None):
        self.temp_dir = temp_dir or Path(tempfile.mkdtemp())
        self.created_files = []

    def create_test_file(
        self, filename: str, content: str = None, subdirectory: str = None
    ) -> Path:
        """Create a test file with optional content."""
        if subdirectory:
            file_dir = self.temp_dir / subdirectory
            file_dir.mkdir(parents=True, exist_ok=True)
        else:
            file_dir = self.temp_dir

        file_path = file_dir / filename

        if content is None:
            content = f"Test content for {filename}"

        file_path.write_text(content, encoding="utf-8")
        self.created_files.append(file_path)

        return file_path

    def create_test_directory(self, dirname: str) -> Path:
        """Create a test directory."""
        dir_path = self.temp_dir / dirname
        dir_path.mkdir(parents=True, exist_ok=True)
        return dir_path

    def cleanup(self):
        """Clean up created files and directories."""
        import shutil

        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)


class AsyncTestHelper:
    """Helper for async testing operations."""

    @staticmethod
    async def wait_for_condition(
        condition_func, timeout: float = 10.0, interval: float = 0.1
    ) -> bool:
        """Wait for a condition to become true."""
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                if await condition_func():
                    return True
            except Exception:
                pass  # Ignore exceptions during condition checking
            await asyncio.sleep(interval)
        return False

    @staticmethod
    async def run_with_timeout(coro, timeout: float = 30.0):
        """Run a coroutine with timeout."""
        try:
            return await asyncio.wait_for(coro, timeout=timeout)
        except asyncio.TimeoutError:
            pytest.fail(f"Operation timed out after {timeout} seconds")

    @staticmethod
    async def gather_with_concurrency(
        coroutines: List, max_concurrency: int = 5
    ) -> List[Any]:
        """Run coroutines with limited concurrency."""
        semaphore = asyncio.Semaphore(max_concurrency)

        async def run_with_semaphore(coro):
            async with semaphore:
                return await coro

        return await asyncio.gather(
            *[run_with_semaphore(coro) for coro in coroutines], return_exceptions=True
        )


class PerformanceTestHelper:
    """Helper for performance testing."""

    def __init__(self):
        self.metrics = {}
        self.start_time = None

    def start_measurement(self):
        """Start performance measurement."""
        import psutil

        self.start_time = time.time()
        self.start_memory = psutil.virtual_memory().used
        self.start_cpu = psutil.cpu_percent()

    def stop_measurement(self) -> Dict[str, float]:
        """Stop measurement and return metrics."""
        import psutil

        if self.start_time is None:
            raise ValueError("Measurement not started")

        end_time = time.time()
        end_memory = psutil.virtual_memory().used
        end_cpu = psutil.cpu_percent()

        self.metrics = {
            "execution_time": end_time - self.start_time,
            "memory_delta_mb": (end_memory - self.start_memory) / (1024 * 1024),
            "avg_cpu_percent": (self.start_cpu + end_cpu) / 2,
        }

        return self.metrics

    @staticmethod
    async def measure_async_operation(coro) -> Dict[str, Any]:
        """Measure performance of an async operation."""
        import psutil

        start_time = time.time()
        start_memory = psutil.virtual_memory().used

        try:
            result = await coro
            success = True
            error = None
        except Exception as e:
            result = None
            success = False
            error = str(e)

        end_time = time.time()
        end_memory = psutil.virtual_memory().used

        return {
            "result": result,
            "success": success,
            "error": error,
            "execution_time": end_time - start_time,
            "memory_delta_mb": (end_memory - start_memory) / (1024 * 1024),
        }


class SecurityTestHelper:
    """Helper for security testing."""

    @staticmethod
    def get_injection_payloads() -> Dict[str, List[str]]:
        """Get common injection attack payloads."""
        return {
            "sql_injection": [
                "'; DROP TABLE users; --",
                "1' OR '1'='1",
                "admin'--",
                "' UNION SELECT * FROM users --",
                "1; DELETE FROM users WHERE 1=1 --",
            ],
            "xss": [
                "<script>alert('xss')</script>",
                "javascript:alert('xss')",
                "<img src=x onerror=alert('xss')>",
                "';alert('xss');//",
                "<svg onload=alert('xss')>",
            ],
            "command_injection": [
                "; ls -la",
                "| cat /etc/passwd",
                "&& rm -rf /",
                "`whoami`",
                "$(id)",
            ],
            "path_traversal": [
                "../../../etc/passwd",
                "..\\..\\..\\windows\\system32\\config\\sam",
                "....//....//....//etc/passwd",
                "%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd",
            ],
        }

    @staticmethod
    def check_sensitive_data_patterns(text: str) -> List[str]:
        """Check for sensitive data patterns in text."""
        import re

        patterns = {
            "email": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
            "credit_card": r"\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b",
            "ssn": r"\b\d{3}-\d{2}-\d{4}\b",
            "api_key": r"(?i)(api[_-]?key|secret|token)\s*[:=]\s*['\"]?([a-zA-Z0-9_-]+)['\"]?",
            "password": r"(?i)password\s*[:=]\s*['\"]?([^\s'\"]+)['\"]?",
        }

        found_patterns = []
        for pattern_name, pattern in patterns.items():
            matches = re.findall(pattern, text)
            if matches:
                found_patterns.append(f"{pattern_name}: {matches}")

        return found_patterns


class TestEnvironmentManager:
    """Manage test environment setup and cleanup."""

    def __init__(self):
        self.temp_dirs = []
        self.mock_servers = []
        self.cleanup_functions = []

    def create_temp_directory(self) -> Path:
        """Create a temporary directory."""
        temp_dir = Path(tempfile.mkdtemp(prefix="openmanus_test_"))
        self.temp_dirs.append(temp_dir)
        return temp_dir

    def register_cleanup(self, cleanup_func):
        """Register a cleanup function."""
        self.cleanup_functions.append(cleanup_func)

    def cleanup_all(self):
        """Clean up all resources."""
        # Run custom cleanup functions
        for cleanup_func in self.cleanup_functions:
            try:
                cleanup_func()
            except Exception as e:
                print(f"Error during cleanup: {e}")

        # Clean up temporary directories
        import shutil

        for temp_dir in self.temp_dirs:
            if temp_dir.exists():
                try:
                    shutil.rmtree(temp_dir)
                except Exception as e:
                    print(f"Error cleaning up {temp_dir}: {e}")

        # Stop mock servers
        for server in self.mock_servers:
            try:
                if hasattr(server, "stop"):
                    server.stop()
            except Exception as e:
                print(f"Error stopping mock server: {e}")


# Pytest fixtures using the helpers


@pytest.fixture
def test_data_generator():
    """Test data generator fixture."""
    return TestDataGenerator()


@pytest.fixture
def mock_factory():
    """Mock factory fixture."""
    return MockFactory()


@pytest.fixture
def file_test_helper(temp_workspace):
    """File test helper fixture."""
    helper = FileTestHelper(temp_workspace)
    yield helper
    helper.cleanup()


@pytest.fixture
def async_test_helper():
    """Async test helper fixture."""
    return AsyncTestHelper()


@pytest.fixture
def performance_test_helper():
    """Performance test helper fixture."""
    return PerformanceTestHelper()


@pytest.fixture
def security_test_helper():
    """Security test helper fixture."""
    return SecurityTestHelper()


@pytest.fixture
def test_env_manager():
    """Test environment manager fixture."""
    manager = TestEnvironmentManager()
    yield manager
    manager.cleanup_all()
